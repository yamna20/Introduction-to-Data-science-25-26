# -*- coding: utf-8 -*-
"""0h6290UrOynin046kj6BJEEhujdZTP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1f2hzNNtKEb8Yb6Z5wZXbsJLcyImsRTZn

# Introduction to Data Science 2025

# Week 1

## Exercise 1 | Matrix warm-up
<span style="background-color: #ccfff2"> *Note: You can find tutorials for NumPy and Pandas under 'Useful tutorials' in the course material.*</span>

One of the most useful properties of any scientific programming language (Python with NumPy, R, Julia, etc) is that they allow us to work with matrices efficiently. Let's learn more about these features!

### 1.1 Basics

1. Let's start by creating two arrays <span style="background-color: #ccfff2"> A</span> and <span style="background-color: #ccfff2"> B</span> which each have the integers <span style="background-color: #ccfff2"> 0, 1, 2, ..., 1e7-1</span>. Use the normal arrays or lists of the programming language you are using, e.g. *list* or *[ ]* or *numpy.array()* in Python.
"""

import numpy as np

# NumPy arrays
A = np.arange(int(1e7))
B = np.arange(int(1e7))

print(A[:10])  # first 10 elements
print(B[:10])

"""2. Create a function that uses a <span style="background-color: #ccfff2"> for loop</span> or equivalent to return a new array <span style="background-color: #ccfff2"> C</span>, which contains the <span style="background-color: #ccfff2"> element-wise sum of *A* and *B*</span>, e.g. C should contain the integers <span style="background-color: #ccfff2"> 0, 2, 4, etc</span>."""

def elementwise_sum(A, B):
    C = []
    for i in range(len(A)):
        C.append(A[i] + B[i])
    return C

"""3. Next, let's create another function that uses NumPy (or equivalent) to do the same. To try it out, allocate two arrays (e.g. using <span style="background-color: #ccfff2"> np.array</span> in NumPy) and add the arrays together using your function. Don't use loops, instead, find out how to add the two arrays directly. What do you notice in comparison to the previous function?"""

import numpy as np

def elementwise_sum_numpy(A, B):
    return A + B

"""### 1.2 Array manipulation

<span style="background-color: #ccfff2"> *Note: for the following exercises, only use NumPy or equivalent functions. Don't use any loops.* </span>
1. Create the following array:

*[hint: <span style="background-color: #ccfff2"> np.reshape</span>]*
"""

# array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],
#        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],
#        [20, 21, 22, 23, 24, 25, 26, 27, 28, 29],
#        [30, 31, 32, 33, 34, 35, 36, 37, 38, 39],
#        [40, 41, 42, 43, 44, 45, 46, 47, 48, 49],
#        [50, 51, 52, 53, 54, 55, 56, 57, 58, 59],
#        [60, 61, 62, 63, 64, 65, 66, 67, 68, 69],
#        [70, 71, 72, 73, 74, 75, 76, 77, 78, 79],
#        [80, 81, 82, 83, 84, 85, 86, 87, 88, 89],
#        [90, 91, 92, 93, 94, 95, 96, 97, 98, 99]])

import numpy as np

# Create numbers from 0 to 99
arr = np.arange(100)

# Reshape into 10x10
matrix_10x10 = arr.reshape(10, 10)

print(matrix_10x10)

"""2. Create the following array:"""

# array([[0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],
#        [0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],
#        [0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],
#        [0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],
#        [0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],
#        [0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],
#        [0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],
#        [0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],
#        [0., 1., 0., 1., 0., 1., 0., 1., 0., 1.],
#        [0., 1., 0., 1., 0., 1., 0., 1., 0., 1.]])

import numpy as np

# Create a single row pattern
row = np.array([0. , 1.] * 5)   # length 10 â†’ [0,1,0,1,0,1,0,1,0,1]

# Repeat this row 10 times to make a 10x10 matrix
checkerboard = np.tile(row, (10, 1))

print(checkerboard)

"""3. Create the following array (D):"""

# array([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],
#        [1., 0., 1., 1., 1., 1., 1., 1., 1., 1.],
#        [1., 1., 0., 1., 1., 1., 1., 1., 1., 1.],
#        [1., 1., 1., 0., 1., 1., 1., 1., 1., 1.],
#        [1., 1., 1., 1., 0., 1., 1., 1., 1., 1.],
#        [1., 1., 1., 1., 1., 0., 1., 1., 1., 1.],
#        [1., 1., 1., 1., 1., 1., 0., 1., 1., 1.],
#        [1., 1., 1., 1., 1., 1., 1., 0., 1., 1.],
#        [1., 1., 1., 1., 1., 1., 1., 1., 0., 1.],
#        [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.]])

import numpy as np

# Start with all ones
arr = np.ones((10, 10))

# Set diagonal to 0
np.fill_diagonal(arr, 0)

print(arr)

"""4. Create the following array (E):"""

# array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],
#        [1., 1., 1., 1., 1., 1., 1., 1., 0., 1.],
#        [1., 1., 1., 1., 1., 1., 1., 0., 1., 1.],
#        [1., 1., 1., 1., 1., 1., 0., 1., 1., 1.],
#        [1., 1., 1., 1., 1., 0., 1., 1., 1., 1.],
#        [1., 1., 1., 1., 0., 1., 1., 1., 1., 1.],
#        [1., 1., 1., 0., 1., 1., 1., 1., 1., 1.],
#        [1., 1., 0., 1., 1., 1., 1., 1., 1., 1.],
#        [1., 0., 1., 1., 1., 1., 1., 1., 1., 1.],
#        [0., 1., 1., 1., 1., 1., 1., 1., 1., 1.]])

import numpy as np

n = 10  # size of the array
arr = np.ones((n, n), dtype=float)  # start with all ones

# place zeros on the anti-diagonal
for i in range(n):
    arr[i, n-1-i] = 0

print(arr)

"""5. Call the last two matrices <span style="background-color: #ccfff2">D</span> and <span style="background-color: #ccfff2">E</span>, respectively. Show that the determinant of their product (matrix multiplication) is the same as the product of their determinants. That is calculate both <span style="background-color: #ccfff2">det(DE)</span> and <span style="background-color: #ccfff2">det(D) * det(E)</span>, and show that they are the same. Is it a coincidence? (I think not) The product of the determinants (or the determinant of the product) should be -81."""

import numpy as np

# Define D (diagonal zeros, ones elsewhere)
n = 10
D = np.ones((n, n), dtype=float)
np.fill_diagonal(D, 0)

# Define E (anti-diagonal zeros, ones elsewhere)
E = 1 - np.fliplr(np.eye(n))

# Compute product
DE = np.dot(D, E)

# Compute determinants
det_DE = np.linalg.det(DE)
det_D = np.linalg.det(D)
det_E = np.linalg.det(E)

print("det(DE) =", det_DE)
print("det(D)*det(E) =", det_D * det_E)

"""*italicized text:*-- *Use this markdown cell for your written answer* --

This is not a coincidence that both of these multiplications are equal but it is a property of matrix multiplication

### 1.3 Slicing

Array slicing is a powerful way to extract data from an array. Let's practice array slicing with the following exercises!

1. Load the [California housing dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html). The data should be a matrix of shape <span style="background-color: #ccfff2">(20640, 8)</span>, that is 20640 rows and 8 columns. Use the <span style="background-color: #ccfff2">.shape</span> attribute of NumPy arrays to verify this. Here's a [description of the fields](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset).
"""

from sklearn.datasets import fetch_california_housing
import numpy as np

# Load the dataset
housing = fetch_california_housing()
data = housing.data  # shape should be (20640, 8)

# Convert to NumPy array (just to be sure)
data = np.array(data)

# Verify the shape
print("Shape of the dataset:", data.shape)

"""2. Select rows where the average number of bedrooms <span style="background-color: #ccfff2">(AveBedrms)</span> is higher than 2. The first few row indices should be <span style="background-color: #ccfff2">710,  1023,  1024, ...</span> (zero-indexed). Count these houses - how many rows are selected? *[hint: <span style="background-color: #ccfff2">np.where</span>]*"""

import numpy as np

# Find indices where AveBedrms > 2
indices = np.where(data[:, 3] > 2)[0]  # [0] to get the array of indices
print("First few row indices:", indices[:300])

# Count how many rows satisfy the condition
count = len(indices)
print("Number of rows with AveBedrms > 2:", count)

"""3. Select the rows where the median house age (i.e. median in each block group) <span style="background-color: #ccfff2">(HouseAge)</span> is between 1 and 3 years (inclusive). There should be **124** of these."""

import numpy as np

# Select rows where 1 <= HouseAge <= 3
indices = np.where((data[:, 1] >= 1) & (data[:, 1] <= 3))[0]

# Count how many rows satisfy the condition
count = len(indices)
print("Number of rows with HouseAge between 1 and 3:", count)

# Optional: show the first few row indices
print("First few row indices:", indices[:150])

"""4. Find the mean of the block group population <span style="background-color: #ccfff2">(Population)</span> for homes whose median value is more than 25000 USD (the target variable). It should be around **1425.68**."""

from sklearn.datasets import fetch_california_housing
import numpy as np

# Load the dataset
housing = fetch_california_housing()
data = housing.data      # shape (20640, 8)
target = housing.target  # median house value (in 100,000s of USD)

# Threshold in correct units
threshold = 0.25  # 25,000 USD

# Boolean mask
mask = target > threshold

# Extract Population for these rows
population_selected = data[mask, 4]

mean_population = np.mean(population_selected)
print("Mean population for homes with median value > 25000 USD:", mean_population)
print(population_selected[:100])

"""## Exercise 2 | Working with text data

Next, let's look into some text data. We will be looking into Amazon reviews, and the necessary steps to transform a raw dataset into a format more suitable for prediction tasks.

1. Download the automotive 5-core dataset from [here](https://jmcauley.ucsd.edu/data/amazon_v2/categoryFilesSmall/Automotive_5.json.gz). Next, you can extract the data in <span style="background-color: #ccfff2">JSON</span> format. You can also download one of the bigger ones, if you are feeling ambitious. Open the JSON file. Access the <span style="background-color: #ccfff2">reviewText</span> field, which contains the unstructured review text written by the user.

For instance, the first review reads as follows:

*'After I wrote the below review, the manufacturer contacted me and explained how to use this.  Instead of the (current) picture on Amazon where the phone is placed vertically, you actually use the stand with the phone placed horizontally. [...]'*
"""

import gzip
import json
filename = "/content/Automotive_5.json.gz"

reviews = []
with gzip.open(filename, 'rt', encoding='utf-8') as f:  # 'rt' = read as text
    for i, line in enumerate(f):
        if i >= 1000:  # optional: read only first 1000 reviews to speed up
            break
        review = json.loads(line)  # convert JSON string to Python dict
        reviews.append(review)
# First review
first_review_text = reviews[0]['reviewText']
print("First review:\n", first_review_text)

# Optional: get all review texts into a list
all_review_texts = [r['reviewText'] for r in reviews]
for i, text in enumerate(all_review_texts[:5]):
    print(f"Review {i+1}:\n{text}\n")

"""2. Next, let's follow some steps to normalize the text data.

When dealing with natural language, it is important to notice that while, for example, the words "Copper" and "copper" are represented by two different strings, they have the same meaning. When applying statistical methods on this data, it is useful to ensure that words with the same meaning are represented by the same string.

* <span style="background-color: #ccfff2">Downcasing</span>: Let's first downcase the contents of the <span style="background-color: #ccfff2">reviewText</span> field.

Now the first review should be:

*'after i wrote the below review, the manufacturer contacted me and explained how to use this.  instead of the (current) picture on amazon where the phone is placed vertically, you actually use the stand with the phone placed horizontally.'*
"""

# Downcase all review texts
all_review_texts_lower = [r['reviewText'].lower() for r in reviews]

# Example: first review after downcasing
print(all_review_texts_lower[0])

"""3. Let's continue with punctuation and stop word removal. Stop words are words like "and", "the", etc. They are usually very common words that have little to do with the actual content matter. There's plenty openly available lists of stop words for almost any (natural) language.

* <span style="background-color: #ccfff2">Punctuation and stop-word removal</span>: Let's now remove all punctuation, as well as the stop words. You can find a stop word list for English, e.g. [here](https://gist.github.com/xldrkp/4a3b1a33f10d37bedbe0068f2b4482e8#file-stopwords-en-txt).*(use the link to download a txt of english stopwords)* Save the stopwords in the file as "stopwords-en.txt".

First review at this point reads as:

*'wrote review manufacturer contacted explained current picture amazon phone vertically stand phone horizontally'*
"""

# Colab: punctuation + stopword removal for Automotive_5.json.gz
import gzip
import json
import re
from pathlib import Path

DATA_PATH = "/content/Automotive_5.json.gz"
STOPWORDS_PATH = "/content/stopwords-en.txt"


# 1) Load stopwords (one word per line)

stopwords_path = Path(STOPWORDS_PATH)
if not stopwords_path.exists():
    raise FileNotFoundError(f"Stopwords file not found: {STOPWORDS_PATH}")

with open(stopwords_path, 'r', encoding='utf-8') as f:
    stopwords = set(line.strip().lower() for line in f if line.strip())

# Optionally ensure very short tokens (single letters) are removed:
# (many stopword lists already include 'a' and 'i', but this guarantees removal)
# We won't add common words automatically; we only filter tokens of length 1 here.
# stopwords.update({'a','i'})   # uncomment if your stopwords file doesn't include them


# 2) Cleaning function

def clean_text(text, stopwords):
    """Lowercase -> remove non-letters -> collapse spaces -> remove stopwords -> join."""
    if not isinstance(text, str):
        return ""
    # lowercase
    text = text.lower()
    # replace anything that's not a lowercase letter or whitespace with a space
    text = re.sub(r'[^a-z\s]', ' ', text)
    # collapse multiple spaces and trim
    text = re.sub(r'\s+', ' ', text).strip()
    # split and remove stopwords and single-letter tokens
    tokens = [tok for tok in text.split() if tok not in stopwords and len(tok) > 1]
    return " ".join(tokens)

# 3) Read gzip JSON file and clean the first review found

first_raw = None
first_cleaned = None

with gzip.open(DATA_PATH, 'rt', encoding='utf-8', errors='ignore') as f:
    for line in f:
        # Each line is a separate JSON object in these Amazon datasets
        try:
            obj = json.loads(line)
        except Exception:
            # skip malformed lines
            continue
        if 'reviewText' not in obj:
            continue
        first_raw = obj.get('reviewText', '')
        first_cleaned = clean_text(first_raw, stopwords)
        break  # we only wanted the first review for the exercise

# 4) Print results

if first_raw is None:
    print("No reviewText found in the file (check the file).")
else:
    print("----- Original (first 500 chars) -----\n")
    print(first_raw[:500], "\n\n")
    print("----- Cleaned (stopwords + punctuation removed) -----\n")
    print(first_cleaned, "\n")

"""4. Let's continue with stemming. For example, while the words "swims" and "swim" are different strings, they both refer to swimming. [Stemming](https://en.wikipedia.org/wiki/Stemming) refers to the process of mapping words from their inflected form to their base form, for instance: swims -> swim.

* <span style="background-color: #ccfff2">Stemming</span>: Apply a stemmer on the paragraphs, so that inflected forms are mapped to the base form. For example, for Python the popular natural language toolkit [nltk](http://www.nltk.org/howto/stem.html) has an easy to use stemmer. In case you are using R, you can try the [Snowball stemmer](https://www.rdocumentation.org/packages/corpus/versions/0.10.2/topics/stem_snowball). You can find out how to install nltk from [here](https://www.nltk.org/install.html). It will take a while to run! So, grab a coffee and wait :D

Finally, after stemming:

*'wrote review manufactur contact explain current pictur amazon phone vertic stand phone horizont'*
"""

import nltk
from nltk.stem import PorterStemmer

# Initialize stemmer
stemmer = PorterStemmer()

# Example: clean first review (already lowercased + punctuation + stopwords removed)
cleaned_review = "wrote review manufacturer contacted explained current picture amazon phone vertically stand phone horizontally"

# Apply stemming word by word
stemmed_review = " ".join([stemmer.stem(word) for word in cleaned_review.split()])

print("Before stemming:", cleaned_review)
print("After stemming:", stemmed_review)

"""5. Finally, filter the data by selecting reviews where the field <span style="background-color: #ccfff2">overall</span> is 4 or 5, and store the review texts in a file named <span style="background-color: #ccfff2">pos.txt</span>. Similarly, select reviews with rating 1 or 2 and store them in a file named <span style="background-color: #ccfff2">neg.txt</span>. Ignore the reviews with overall rating 3. Each line in the two files should contain exactly one preprocessed review text without the rating."""

import json
import string
from nltk.stem import PorterStemmer

# File paths
dataset_path = "/content/Automotive_5.json"       # Your JSON dataset
stopwords_path = "/content/stopwords-en.txt"     # Stopwords file

# Step 1: Load stopwords
with open(stopwords_path, "r") as f:
    stopwords = set(f.read().splitlines())

# Step 2: Initialize stemmer
stemmer = PorterStemmer()

# Step 3: Load reviews
reviews = []
with open(dataset_path, "r", encoding="utf-8", errors="ignore") as f:
    for line in f:
        try:
            reviews.append(json.loads(line))
        except:
            continue

print(f"Total reviews loaded: {len(reviews)}")

# Step 4: Filter and preprocess reviews
with open("pos.txt", "w") as pos_file, open("neg.txt", "w") as neg_file:
    for review in reviews:
        text = review.get("reviewText", "").lower()  # downcase
        text = text.translate(str.maketrans("", "", string.punctuation))  # remove punctuation
        words = [stemmer.stem(w) for w in text.split() if w not in stopwords]  # remove stopwords + stemming
        cleaned_text = " ".join(words)

        rating = review.get("overall", 3)
        if rating in [4, 5]:
            pos_file.write(cleaned_text + "\n")
        elif rating in [1, 2]:
            neg_file.write(cleaned_text + "\n")

# Step 5: Quick check
print("First 5 positive reviews:")
!head -n 5 pos.txt
print("\nFirst 5 negative reviews:")
!head -n 5 neg.txt

"""**Remember to submit your solutions. You can return this Jupyter notebook (.ipynb) or .py, .R, etc depending on your programming preferences. Remember to also submit your SQL queries. No need to submit the text files for the programming exercises.**

## Exercise 3 | SQL basics

Next, let's take a refresher on the basics of SQL. In this exercise, you will be working on the simplified Northwind 2000 SQLite database. You can download the database from Kaggle [here](https://courses.mooc.fi/api/v0/files/course/f92ffc32-2dd4-421d-87f3-c48800422cc5/files/VEKX2bxGCDGyojG902gmYZTXCnrAQw.zip).

To test your SQL queries and complete the exercise, you can download and install SQLite if you don't yet have it installed.

Please write SQL queries for the tasks on the simplified Northwind 2000 SQLite database.

1. List the first name, last name, and hire date of all employees hired after January 1st, 1994.

2. Count how many orders each customer has placed.

3. Find the names of all customers who have ordered the product "Chai".

4. Find all orders that have been placed but not yet shipped.

5. Find the customer who has placed the most orders.
"""